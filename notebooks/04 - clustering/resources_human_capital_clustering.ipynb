{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3922c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('resources_human_capital_cleaned.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of countries: {len(df)}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "countries = df['Country']\n",
    "X = df.drop('Country', axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a4707",
   "metadata": {},
   "source": [
    "# 1. FIRST STAGE: K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfa348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal k\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 8)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot to choose k\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(k_range, inertia, 'bo-')\n",
    "axes[0].set_xlabel('Number of clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Scores')\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54686149",
   "metadata": {},
   "source": [
    "# Choose optimal k (based on plots - let's use 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31454183",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "print(f\"\\nK-Means clustering completed with {optimal_k} clusters\")\n",
    "print(f\"Cluster distribution: {np.bincount(kmeans_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b54cd",
   "metadata": {},
   "source": [
    "# 2. SECOND STAGE: ANN FOR CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ANN\n",
    "X_ann = X_scaled.copy()\n",
    "y_ann = kmeans_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcff865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ann, y_ann, test_size=0.2, random_state=42, stratify=y_ann\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=optimal_k)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ANN model\n",
    "def build_ann_model(input_dim, n_clusters):\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(n_clusters, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0094e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "input_dim = X_train.shape[1]\n",
    "model = build_ann_model(input_dim, optimal_k)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nANN Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nTraining ANN...\")\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b27464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. EVALUATE THE ANN CLUSTERING\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION OF ANN CLUSTERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Cluster')\n",
    "plt.ylabel('True Cluster (K-Means)')\n",
    "plt.show()\n",
    "\n",
    "# 4. COMPARE K-MEANS AND ANN CLUSTERING\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON: K-MEANS vs ANN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Predict cluster assignments for all data using ANN\n",
    "all_pred_proba = model.predict(X_ann)\n",
    "all_pred = np.argmax(all_pred_proba, axis=1)\n",
    "\n",
    "# Compare cluster assignments\n",
    "agreement = np.mean(kmeans_labels == all_pred)\n",
    "print(f\"Agreement between K-Means and ANN: {agreement:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'KMeans_Cluster': kmeans_labels,\n",
    "    'ANN_Cluster': all_pred,\n",
    "    'Agreement': kmeans_labels == all_pred\n",
    "})\n",
    "\n",
    "print(f\"\\nDisagreements: {sum(~comparison_df['Agreement'])} out of {len(comparison_df)} countries\")\n",
    "\n",
    "if sum(~comparison_df['Agreement']) > 0:\n",
    "    print(\"\\nCountries with different cluster assignments:\")\n",
    "    disagreements = comparison_df[~comparison_df['Agreement']]\n",
    "    print(disagreements[['Country', 'KMeans_Cluster', 'ANN_Cluster']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e59dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VISUALIZE CLUSTERS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTER VISUALIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_ann)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original K-Means clusters\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0].set_xlabel('PCA Component 1')\n",
    "axes[0].set_ylabel('PCA Component 2')\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# ANN predicted clusters\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=all_pred, \n",
    "                          cmap='plasma', alpha=0.7, s=50)\n",
    "axes[1].set_xlabel('PCA Component 1')\n",
    "axes[1].set_ylabel('PCA Component 2')\n",
    "axes[1].set_title('ANN Predicted Clusters')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# Highlight disagreements\n",
    "colors = ['green' if agree else 'red' for agree in comparison_df['Agreement']]\n",
    "scatter3 = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=50)\n",
    "axes[2].set_xlabel('PCA Component 1')\n",
    "axes[2].set_ylabel('PCA Component 2')\n",
    "axes[2].set_title('Agreement between K-Means and ANN\\n(Green=Agree, Red=Disagree)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e85ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CLUSTER PROFILING AND INTERPRETATION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTER PROFILING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Add cluster assignments to original data\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters['KMeans_Cluster'] = kmeans_labels\n",
    "df_with_clusters['ANN_Cluster'] = all_pred\n",
    "\n",
    "# Analyze each cluster's characteristics\n",
    "def analyze_cluster(cluster_id, method='KMeans'):\n",
    "    cluster_data = df_with_clusters[df_with_clusters[f'{method}_Cluster'] == cluster_id]\n",
    "    \n",
    "    # Select key features for profiling\n",
    "    key_features = [\n",
    "        'Population_millions_2024',\n",
    "        'Population density_people per sq. km_2024',\n",
    "        'Agri_Inputs_Agricultural employment_% of total employment_2020',\n",
    "        'Edu_Participation_Gross enrollment ratio_Tertiary_% of relevant age group_2022',\n",
    "        'Energy_Energy use_per capita_kilograms of oil equivalent_2015',\n",
    "        'Population_Population age composition_Ages 65+_%_2024',\n",
    "        'Labor_Labor force (ages 15 and older)_Total_millions_2021'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{method} Cluster {cluster_id} Analysis:\")\n",
    "    print(f\"Number of countries: {len(cluster_data)}\")\n",
    "    print(f\"Example countries: {', '.join(cluster_data['Country'].head(5).tolist())}\")\n",
    "    \n",
    "    stats = {}\n",
    "    for feature in key_features:\n",
    "        if feature in cluster_data.columns:\n",
    "            stats[feature] = {\n",
    "                'mean': cluster_data[feature].mean(),\n",
    "                'std': cluster_data[feature].std(),\n",
    "                'min': cluster_data[feature].min(),\n",
    "                'max': cluster_data[feature].max()\n",
    "            }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze each K-Means cluster\n",
    "for cluster_id in range(optimal_k):\n",
    "    analyze_cluster(cluster_id, 'KMeans')\n",
    "\n",
    "# 7. FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE FOR CLUSTERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract weights from the ANN to understand feature importance\n",
    "# Get weights from the first layer\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Calculate feature importance as absolute weight sum\n",
    "feature_importance = np.abs(first_layer_weights).sum(axis=1)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features for ANN Clustering:\")\n",
    "print(importance_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ec7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance (Weight Magnitude)')\n",
    "plt.title('Top 15 Features Influencing ANN Clustering Decisions')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. CLUSTER CENTROID ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTER CENTROID ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get cluster centroids from K-Means\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Inverse transform to original scale\n",
    "centroids_original = scaler.inverse_transform(centroids)\n",
    "centroids_df = pd.DataFrame(centroids_original, columns=X.columns)\n",
    "\n",
    "# Select top features for centroid comparison\n",
    "top_features = importance_df['Feature'].head(8).tolist()\n",
    "\n",
    "print(\"\\nCluster Centroids for Top Features:\")\n",
    "centroids_top = centroids_df[top_features]\n",
    "print(centroids_top.round(2))\n",
    "\n",
    "# Visualize centroid differences\n",
    "plt.figure(figsize=(12, 8))\n",
    "centroids_top.T.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Cluster Centroids Comparison for Top Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Centroid Value (Original Scale)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. PREDICTIVE POWER FOR NEW DATA\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL VALIDATION AND PREDICTIVE POWER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cross-validation to test model robustness\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Use simpler ANN for cross-validation (for speed)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(mlp, X_ann, y_ann, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 10. CLUSTER STABILITY ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTER STABILITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test clustering stability with different random seeds\n",
    "stability_results = []\n",
    "for seed in range(5):\n",
    "    kmeans_temp = KMeans(n_clusters=optimal_k, random_state=seed, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_scaled)\n",
    "    \n",
    "    # Train ANN on these labels\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        X_scaled, labels_temp, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    y_train_cat_temp = to_categorical(y_train_temp, num_classes=optimal_k)\n",
    "    y_test_cat_temp = to_categorical(y_test_temp, num_classes=optimal_k)\n",
    "    \n",
    "    model_temp = build_ann_model(input_dim, optimal_k)\n",
    "    model_temp.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history_temp = model_temp.fit(\n",
    "        X_train_temp, y_train_cat_temp,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_temp = np.argmax(model_temp.predict(X_test_temp), axis=1)\n",
    "    accuracy_temp = accuracy_score(y_test_temp, y_pred_temp)\n",
    "    \n",
    "    stability_results.append(accuracy_temp)\n",
    "    print(f\"Seed {seed}: Test Accuracy = {accuracy_temp:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage stability accuracy: {np.mean(stability_results):.4f}\")\n",
    "print(f\"Stability std: {np.std(stability_results):.4f}\")\n",
    "\n",
    "# 11. FINAL RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. **Hybrid K-Means + ANN Approach Benefits:**\")\n",
    "print(\"   - ANN learns complex patterns beyond simple distance metrics\")\n",
    "print(\"   - Model can predict clusters for new data without rerunning K-Means\")\n",
    "print(\"   - Handles non-linear relationships between features\")\n",
    "print(\"   - Provides feature importance insights\")\n",
    "\n",
    "print(\"\\n2. **When to Use This Approach:**\")\n",
    "print(\"   - Large datasets where K-Means alone is computationally expensive\")\n",
    "print(\"   - When you need to assign new points to existing clusters\")\n",
    "print(\"   - When clusters have complex, non-linear boundaries\")\n",
    "print(\"   - When interpretability of clustering decisions is important\")\n",
    "\n",
    "print(\"\\n3. **Limitations:**\")\n",
    "print(\"   - Requires sufficient training data for ANN\")\n",
    "print(\"   - ANN predictions depend on K-Means initial clustering\")\n",
    "print(\"   - More complex than traditional clustering methods\")\n",
    "print(\"   - Risk of overfitting if ANN is too complex\")\n",
    "\n",
    "print(\"\\n4. **For This Dataset:**\")\n",
    "print(f\"   - {optimal_k} clusters identified\")\n",
    "print(f\"   - {accuracy:.2%} agreement between K-Means and ANN\")\n",
    "print(f\"   - Top features: {importance_df['Feature'].head(3).tolist()}\")\n",
    "print(\"   - ANN successfully learned the clustering pattern\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('kmeans_ann_clustering_model.h5')\n",
    "print(\"\\nModel saved as 'kmeans_ann_clustering_model.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
